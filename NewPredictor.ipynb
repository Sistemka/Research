{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from keras.applications import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "from keras.engine import Model\n",
    "from keras.preprocessing import image\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from settings.config import DATA_SET, BASE_DIR, VECTORS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    data_set = []\n",
    "\n",
    "    for root_dir, _, files in os.walk(DATA_SET):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                data_set.append({\n",
    "                    'url': Path(root_dir, file).as_posix(),\n",
    "                    'type': root_dir.split('/')[-1]\n",
    "                })\n",
    "    return pd.DataFrame(data_set)\n",
    "\n",
    "def get_random_file(data_base):\n",
    "    return np.random.choice(data_base.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                     url     type\n",
       "0     /Users/markantipin/Desktop/Systemka/Reserch/Da...    boots\n",
       "1     /Users/markantipin/Desktop/Systemka/Reserch/Da...    boots\n",
       "2     /Users/markantipin/Desktop/Systemka/Reserch/Da...    boots\n",
       "3     /Users/markantipin/Desktop/Systemka/Reserch/Da...    boots\n",
       "4     /Users/markantipin/Desktop/Systemka/Reserch/Da...    boots\n",
       "...                                                 ...      ...\n",
       "1195  /Users/markantipin/Desktop/Systemka/Reserch/Da...  jackets\n",
       "1196  /Users/markantipin/Desktop/Systemka/Reserch/Da...  jackets\n",
       "1197  /Users/markantipin/Desktop/Systemka/Reserch/Da...  jackets\n",
       "1198  /Users/markantipin/Desktop/Systemka/Reserch/Da...  jackets\n",
       "1199  /Users/markantipin/Desktop/Systemka/Reserch/Da...  jackets\n",
       "\n",
       "[1200 rows x 2 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_base.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sparse_matrix(filename, x):\n",
    "    x_coo = x.tocoo()\n",
    "    row = x_coo.row\n",
    "    col = x_coo.col\n",
    "    data = x_coo.data\n",
    "    shape = x_coo.shape\n",
    "    np.savez(filename, row=row, col=col, data=data, shape=shape)\n",
    "\n",
    "\n",
    "def load_sparse_matrix(filename):\n",
    "    y = np.load(filename)\n",
    "    z = sp.coo_matrix((y['data'], (y['row'], y['col'])), shape=y['shape'])\n",
    "    return z\n",
    "\n",
    "\n",
    "def vectorize_all(files, model, px=224, n_dims=512, batch_size=512):\n",
    "    print(\"Will vectorize\")\n",
    "    min_idx = 0\n",
    "    max_idx = min_idx + batch_size\n",
    "    total_max = len(files)\n",
    "    preds = sp.lil_matrix((len(files), n_dims))\n",
    "\n",
    "    print(\"Total: {}\".format(len(files)))\n",
    "    while min_idx < total_max - 1:\n",
    "        print(min_idx)\n",
    "        X = np.zeros(((max_idx - min_idx), px, px, 3))\n",
    "        # For each file in batch, \n",
    "        # load as row into X\n",
    "        i = 0\n",
    "        for i in range(min_idx, max_idx):\n",
    "            file = files[i]\n",
    "            try:\n",
    "                img = image.load_img(file, target_size=(px, px))\n",
    "                img_array = image.img_to_array(img)\n",
    "                X[i - min_idx, :, :, :] = img_array\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        max_idx = i\n",
    "        X = preprocess_input(X)\n",
    "        these_preds = model.predict(X)\n",
    "        shp = ((max_idx - min_idx) + 1, n_dims)\n",
    "        preds[min_idx:max_idx + 1, :] = these_preds.reshape(shp)\n",
    "        min_idx = max_idx\n",
    "        max_idx = np.min((max_idx + batch_size, total_max))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will vectorize\n",
      "Total: 1200\n",
      "0\n",
      "511\n",
      "1022\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG19(weights='imagenet')\n",
    "files = data_base.url\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)\n",
    "vecs = vectorize_all(files, model, n_dims=4096)\n",
    "save_sparse_matrix(VECTORS_PATH, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vectorize(path, model):\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    pred = model.predict(x)\n",
    "    return pred.ravel()\n",
    "\n",
    "\n",
    "def _similar(vec, knn, filenames, n_neighbors=6):\n",
    "    dist, indices = knn.kneighbors(vec.reshape(1, -1), n_neighbors=n_neighbors)\n",
    "    dist, indices = dist.flatten(), indices.flatten()\n",
    "    return [(filenames[indices[i]], dist[i]) for i in range(len(indices))]\n",
    "\n",
    "\n",
    "def load_predictor(data_base):\n",
    "    filenames = data_base.url\n",
    "    vecs = load_sparse_matrix(VECTORS_PATH)\n",
    "    base_model = VGG19(weights='imagenet')\n",
    "    # Read about fc1 here http://cs231n.github.io/convolutional-networks/\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)\n",
    "    knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "    knn.fit(vecs)\n",
    "\n",
    "    def similarity(file_path, n_neighbors=6):\n",
    "        vec = _vectorize(file_path, model)\n",
    "        return _similar(vec, knn, filenames, n_neighbors)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(fnames, origin=None):\n",
    "    if origin is not None:\n",
    "        plt.imshow(Image.open(origin))\n",
    "        plt.axis('off')\n",
    "    plt.figure(figsize=(30, 15))\n",
    "    for i in range(len(fnames)):\n",
    "        f, d = fnames[i]\n",
    "        try:\n",
    "            img = Image.open(f)\n",
    "            plt.subplot(1, 10, i + 1)\n",
    "            plt.axis('off')\n",
    "            plt.title(\"{0:.4f}\".format(d))\n",
    "            plt.imshow(img)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = load_predictor(data_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "s = time()\n",
    "f = get_random_file(data_base)\n",
    "draw(pred(f), f)\n",
    "\n",
    "print(time() - s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/markantipin/Desktop/Systemka/Reserch/DataSet/boots/22.jpg'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
